import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms


MNIST_TYPES = [str(i) for i in range(10)]

# 1. 超参数设置（可在 CLI 中修改）
batch_size = 64
learning_rate = 1e-3
num_epochs = 5

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)


def prompt_mnist_type():
    print("MNIST types:", ", ".join(MNIST_TYPES))
    while True:
        selected = input("Which type to use? (0-9): ").strip()
        if selected in MNIST_TYPES:
            return int(selected)
        print("Invalid type. Please enter a digit from 0 to 9.")


def prompt_hyperparameters():
    global batch_size, learning_rate, num_epochs
    while True:
        print("\nDefault hyperparameters:")
        print("1) batch_size =", batch_size, "(range: 1-1024)")
        print("2) learning_rate =", learning_rate, "(range: 1e-6 to 1.0)")
        print("3) num_epochs =", num_epochs, "(range: 1-100)")

        change = input("Change hyperparameter? (y/n): ").strip().lower()
        if change == "n":
            return
        if change != "y":
            print("Please enter 'y' or 'n'.")
            continue

        choice = input("Which hyperparameter to change? (1-3): ").strip()
        if choice == "1":
            new_val = input("New batch_size (1-1024): ").strip()
            try:
                new_val_int = int(new_val)
            except ValueError:
                print("Invalid integer.")
                continue
            if not (1 <= new_val_int <= 1024):
                print("Out of range.")
                continue
            batch_size = new_val_int
        elif choice == "2":
            new_val = input("New learning_rate (1e-6 to 1.0): ").strip()
            try:
                new_val_float = float(new_val)
            except ValueError:
                print("Invalid float.")
                continue
            if not (1e-6 <= new_val_float <= 1.0):
                print("Out of range.")
                continue
            learning_rate = new_val_float
        elif choice == "3":
            new_val = input("New num_epochs (1-100): ").strip()
            try:
                new_val_int = int(new_val)
            except ValueError:
                print("Invalid integer.")
                continue
            if not (1 <= new_val_int <= 100):
                print("Out of range.")
                continue
            num_epochs = new_val_int
        else:
            print("Invalid choice. Please enter 1, 2, or 3.")
            continue

        print("change complete")


# 2. 数据预处理和 DataLoader
# MNIST 原始是 0~255 的灰度值，我们需要转为 Tensor 并做标准化
transform = transforms.Compose([
    transforms.ToTensor(),                      # [0, 255] -> [0.0, 1.0]
    transforms.Normalize((0.1307,), (0.3081,))  # 按官方均值/方差标准化
])

train_dataset = datasets.MNIST(
    root="./data",
    train=True,
    transform=transform,
    download=True
)

test_dataset = datasets.MNIST(
    root="./data",
    train=False,
    transform=transform,
    download=True
)

# 3. 定义模型：简单 CNN
# 输入：1 x 28 x 28 灰度图
# 输出：10 类 logits
class SimpleCNN(nn.Module):
    def __init__(self):
        super().__init__()
        # 卷积层部分
        self.conv_layers = nn.Sequential(
            nn.Conv2d(1, 32, kernel_size=3, padding=1),  # 输出: 32 x 28 x 28
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),       # 输出: 32 x 14 x 14

            nn.Conv2d(32, 64, kernel_size=3, padding=1), # 输出: 64 x 14 x 14
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2)        # 输出: 64 x 7 x 7
        )

        # 全连接层部分
        self.fc_layers = nn.Sequential(
            nn.Linear(64 * 7 * 7, 128),
            nn.ReLU(),
            nn.Linear(128, 10)  # 输出 logits，未做 softmax
        )

    def forward(self, x):
        # x: (batch_size, 1, 28, 28)
        x = self.conv_layers(x)           # 卷积+池化堆叠
        x = x.view(x.size(0), -1)         # 展平为 (batch_size, 64*7*7)
        x = self.fc_layers(x)             # 线性层
        return x                          # 返回 logits


# 5. 训练一个 epoch 的函数
def train_one_epoch(model, dataloader, criterion, optimizer, device):
    model.train()   # 切换到训练模式（启用 dropout / BN 的训练行为）
    running_loss = 0.0
    correct = 0
    total = 0

    for batch_idx, (inputs, targets) in enumerate(dataloader):
        inputs = inputs.to(device)
        targets = targets.to(device)

        # 1) 梯度清零
        optimizer.zero_grad()

        # 2) 前向传播
        outputs = model(inputs)  # shape: (batch, 10)

        # 3) 计算损失
        loss = criterion(outputs, targets)

        # 4) 反向传播，计算梯度
        loss.backward()

        # 5) 根据梯度更新参数
        optimizer.step()

        # 统计训练状态
        running_loss += loss.item() * inputs.size(0)

        # outputs 是 logits，取最大值所在的类别
        _, predicted = outputs.max(1)
        total += targets.size(0)
        correct += predicted.eq(targets).sum().item()

    epoch_loss = running_loss / total
    epoch_acc = correct / total
    return epoch_loss, epoch_acc


# 6. 在验证/测试集上评估
@torch.no_grad()  # 评估时不需要梯度，节省显存与计算
def evaluate(model, dataloader, criterion, device):
    model.eval()   # 切换到 eval 模式（关闭 dropout / 使用 BN 的 running stats）
    running_loss = 0.0
    correct = 0
    total = 0

    for inputs, targets in dataloader:
        inputs = inputs.to(device)
        targets = targets.to(device)

        outputs = model(inputs)
        loss = criterion(outputs, targets)

        running_loss += loss.item() * inputs.size(0)
        _, predicted = outputs.max(1)
        total += targets.size(0)
        correct += predicted.eq(targets).sum().item()

    epoch_loss = running_loss / total
    epoch_acc = correct / total
    return epoch_loss, epoch_acc


def filter_dataset_by_digit(dataset, digit):
    indices = []
    for idx in range(len(dataset)):
        _, target = dataset[idx]
        if target == digit:
            indices.append(idx)
    return torch.utils.data.Subset(dataset, indices)


def main():
    selected_digit = prompt_mnist_type()
    prompt_hyperparameters()

    train_subset = filter_dataset_by_digit(train_dataset, selected_digit)
    test_subset = filter_dataset_by_digit(test_dataset, selected_digit)

    train_loader = DataLoader(
        dataset=train_subset,
        batch_size=batch_size,
        shuffle=True
    )
    test_loader = DataLoader(
        dataset=test_subset,
        batch_size=batch_size,
        shuffle=False
    )

    model = SimpleCNN().to(device)
    print(model)

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)

    print("model is training, please wait")

    best_test_acc = 0.0
    for epoch in range(num_epochs):
        train_loss, train_acc = train_one_epoch(
            model, train_loader, criterion, optimizer, device
        )
        test_loss, test_acc = evaluate(
            model, test_loader, criterion, device
        )

        print(
            f"Epoch [{epoch+1}/{num_epochs}] "
            f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} "
            f"| Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}"
        )

        if test_acc > best_test_acc:
            best_test_acc = test_acc
            torch.save(model.state_dict(), "best_mnist_cnn.pth")
            print(f"  -> New best model saved with acc {best_test_acc:.4f}")

    print("training completed")
    print(f"Accuracy for type {selected_digit}: {best_test_acc:.4f}")


if __name__ == "__main__":
    main()
