import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms


# 1. 超参数设置
batch_size = 64
learning_rate = 1e-3
num_epochs = 5

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)


# 2. 数据预处理和 DataLoader
# MNIST 原始是 0~255 的灰度值，我们需要转为 Tensor 并做标准化
transform = transforms.Compose([
    transforms.ToTensor(),                      # [0, 255] -> [0.0, 1.0]
    transforms.Normalize((0.1307,), (0.3081,))  # 按官方均值/方差标准化
])

train_dataset = datasets.MNIST(
    root="./data",
    train=True,
    transform=transform,
    download=True
)

test_dataset = datasets.MNIST(
    root="./data",
    train=False,
    transform=transform,
    download=True
)

train_loader = DataLoader(
    dataset=train_dataset,
    batch_size=batch_size,
    shuffle=True
)

test_loader = DataLoader(
    dataset=test_dataset,
    batch_size=batch_size,
    shuffle=False
)


# 3. 定义模型：简单 CNN
# 输入：1 x 28 x 28 灰度图
# 输出：10 类 logits
class SimpleCNN(nn.Module):
    def __init__(self):
        super().__init__()
        # 卷积层部分
        self.conv_layers = nn.Sequential(
            nn.Conv2d(1, 32, kernel_size=3, padding=1),  # 输出: 32 x 28 x 28
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),       # 输出: 32 x 14 x 14

            nn.Conv2d(32, 64, kernel_size=3, padding=1), # 输出: 64 x 14 x 14
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2)        # 输出: 64 x 7 x 7
        )

        # 全连接层部分
        self.fc_layers = nn.Sequential(
            nn.Linear(64 * 7 * 7, 128),
            nn.ReLU(),
            nn.Linear(128, 10)  # 输出 logits，未做 softmax
        )

    def forward(self, x):
        # x: (batch_size, 1, 28, 28)
        x = self.conv_layers(x)           # 卷积+池化堆叠
        x = x.view(x.size(0), -1)         # 展平为 (batch_size, 64*7*7)
        x = self.fc_layers(x)             # 线性层
        return x                          # 返回 logits


model = SimpleCNN().to(device)
print(model)


# 4. 损失函数和优化器
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)


# 5. 训练一个 epoch 的函数
def train_one_epoch(model, dataloader, criterion, optimizer, device):
    model.train()   # 切换到训练模式（启用 dropout / BN 的训练行为）
    running_loss = 0.0
    correct = 0
    total = 0

    for batch_idx, (inputs, targets) in enumerate(dataloader):
        inputs = inputs.to(device)
        targets = targets.to(device)

        # 1) 梯度清零
        optimizer.zero_grad()

        # 2) 前向传播
        outputs = model(inputs)  # shape: (batch, 10)

        # 3) 计算损失
        loss = criterion(outputs, targets)

        # 4) 反向传播，计算梯度
        loss.backward()

        # 5) 根据梯度更新参数
        optimizer.step()

        # 统计训练状态
        running_loss += loss.item() * inputs.size(0)

        # outputs 是 logits，取最大值所在的类别
        _, predicted = outputs.max(1)
        total += targets.size(0)
        correct += predicted.eq(targets).sum().item()

    epoch_loss = running_loss / total
    epoch_acc = correct / total
    return epoch_loss, epoch_acc


# 6. 在验证/测试集上评估
@torch.no_grad()  # 评估时不需要梯度，节省显存与计算
def evaluate(model, dataloader, criterion, device):
    model.eval()   # 切换到 eval 模式（关闭 dropout / 使用 BN 的 running stats）
    running_loss = 0.0
    correct = 0
    total = 0

    for inputs, targets in dataloader:
        inputs = inputs.to(device)
        targets = targets.to(device)

        outputs = model(inputs)
        loss = criterion(outputs, targets)

        running_loss += loss.item() * inputs.size(0)
        _, predicted = outputs.max(1)
        total += targets.size(0)
        correct += predicted.eq(targets).sum().item()

    epoch_loss = running_loss / total
    epoch_acc = correct / total
    return epoch_loss, epoch_acc


# 7. 主训练循环
best_test_acc = 0.0

for epoch in range(num_epochs):
    train_loss, train_acc = train_one_epoch(
        model, train_loader, criterion, optimizer, device
    )
    test_loss, test_acc = evaluate(
        model, test_loader, criterion, device
    )

    print(
        f"Epoch [{epoch+1}/{num_epochs}] "
        f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} "
        f"| Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}"
    )

    # 简单保存“最好”的模型
    if test_acc > best_test_acc:
        best_test_acc = test_acc
        torch.save(model.state_dict(), "best_mnist_cnn.pth")
        print(f"  -> New best model saved with acc {best_test_acc:.4f}")


print("Training finished. Best Test Acc:", best_test_acc)


# 8. 演示如何加载已保存的模型权重
def load_trained_model(weight_path, device):
    model = SimpleCNN().to(device)
    state_dict = torch.load(weight_path, map_location=device)
    model.load_state_dict(state_dict)
    model.eval()
    return model


# 可选：加载后对单个样本做预测
if __name__ == "__main__":
    # 示例：从测试集中拿一个 batch，看前 8 张的预测
    example_loader = DataLoader(test_dataset, batch_size=8, shuffle=True)
    images, labels = next(iter(example_loader))
    images = images.to(device)
    labels = labels.to(device)

    # 加载最优模型
    model = load_trained_model("best_mnist_cnn.pth", device)

    with torch.no_grad():
        outputs = model(images)
        _, predicted = outputs.max(1)

    print("Ground truth labels: ", labels.cpu().numpy().tolist())
    print("Predicted labels:    ", predicted.cpu().numpy().tolist())
